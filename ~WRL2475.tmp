Introduction and problem description:

Over the past few years, opinion polls have been an ever-increasing field of politicians who want to know public opinion about them through receiving feedback, which can help them improve their functioning, because of their desire to please the public at large and their constituency in particular. For this reason, such surveys have become extremely popular in general and especially during election periods. Such opinion polls are usually based on traditional methods such as telephone interviews, face-to-face interviews, street surveys, self-filling questionnaires in the mail, etc. The advancement of technology over the last decade has created new sources from which to obtain particularly large amounts of opinions on political and current issues, from sources such as social networks, news sites and blogs, the amount of data that can be extracted is infinitely greater than traditional methods.
Entering the Big Data era has brought with it a variety of methods for dealing with and analyzing such large amounts of information, from which insights and conclusions can be drawn, such as public opinion trends and public opinion on political issues. These include reviews of news and current events involving politicians on a daily basis, which may vary according to their functioning and influence the popularity they gain in the public.
In this project, the problem we will be addressing is building a personal popularity index for four key politicians, which varies over time based on user reactions to political events in which they are reviewed. These metrics represented positive, negative, and neutral opinions about them. This will be done based on the gathering and analysis of numerous responses to political / current affairs articles published in the New York Times in the United States from 2017-2018. To build the popularity metrics of every politician, we will analyze the responses of these articles to determine whether they are positive, negative or neutral. By analyzing a large number of responses, far greater than any traditional survey, we can formulate a public opinion weighted on the politician in question and build for him a measure of popularity that varies over time according to the publications he is involved in, expressing public opinion about his functioning.

The data we used:

We used three different types of data in this project, with each type being designed for different use. We'll explain what types we used with an explanation of each:
1. All articles and articles in the New York Times during January-May 2017 as well as January-April 2018. These are nine CSV files that describe all the articles and articles published during these months in the newspaper. Each file contains entries that each represent an article that was published that month, and each article details such as its unique identifier number, site publication date, title, web address, and associated list of indications. Each such file has an average of 1000 entries representing 1000 articles. A total of 9 such months total about 9000 articles published in the newspaper during this time period. Weight of each file averages 509KB, total data volume is 4,581KB.

2. All comments posted by users to articles and articles published in the New York Times during January-May 2017 and January-April 2018. These are nine CSV files describing all comments posted by users to all articles and articles published in the newspaper in the months mentioned above. Each entry represents a comment that a user wrote to the article, with the exception of the comment language itself, the date of publication of the comment, a unique ID of the article for which the comment was posted, a unique ID of the comment, and the geographic location (U.S. city and state) of the author who wrote the comment. Average records totaling 2,250,000 responses to articles published in the months listed above, the responses to these files are the source through which we build the politicians' popularity metrics. Weight of each file averages MB160, total data volume in this section 1.43GB.

The above data is at https://www.kaggle.com/aashita/nyt-comments.

3. A tagged data of Twitter comments. These are 1.6 million (1,600,000) tweets of comment on political / current / news topics gathered through the Twitter API. Each entry in this repository contains a tweet that is a response to some political / topical issue as well as tagging a number that indicates whether it is a positive or negative response. The responses in these files are the source of the classified training we will use and their accuracy test. Total data volume in this part MB 233.
This repository is at https://www.kaggle.com/kazanova/sentiment140.
Total data volume used is 1.66GB.

Our Solution:

As mentioned in the introduction, in this project we will analyze user responses to public opinion and to formulate general opinions about elected politicians, from which we would like to build popularity metrics that change over time. To that end, we have taken two major approaches to analyzing responses: the first is lexicon-based and the second is machine-learning-based. Before describing each, first, I describe how the relevant responses we want to analyze are collected from the Datasets mentioned above.
First of all, we extracted from Dataset # 1 all the articles related to the politicians we would like to analyze, which deal with the events they are involved in which have political influence. Each entry representing an article has a field containing indicative tags associated with the article. If the article deals with or is related to a particular politician or the events he is involved in, the name will be included in the article's tagline. We will build such a tag list for every politician and scan all records in the data files for articles containing tags related to it. If an article contains one of the tags, we will save the identifying number for that article. In this way, we will keep all the identifying numbers of articles published in a particular month related to the politician of our choice.
Then, we will extract the user's responses to relevant articles for the politician, using the identifying numbers we have saved, and then scan the comment files in Dataset # 2. Each record representing a comment has a field with a unique identifier representing its article. By listing the IDs we have saved, we will collect all user responses to articles whose ID was in the ID list. In this way, we will gather all of the user comments to articles related to the elected politician.
In order to create a time-varying popularity index, for each politician, the following metrics for each method (lexicon or ML) were calculated separately:
1. General Monthly Popularity Index by analyzing the responses collected on a particular politician over a full month period (months are January-May 2017 as well as January-April 2018). This is done through a serial pass on the reactions related to the elected politician who was received from the process described earlier. For each such response, we will classify the emotion expressed therein according to one of the classification methods we will elaborate on later. In the lexicon-based method the classification moves on a scale of values of very positive emotion, slightly positive, neutral, slightly negative or very negative. In the ML based classification the emotion moves on a positive, neutral and negative value scale. At the end of the calculations, the distribution of opinions for that month is calculated and displayed as a pie chart.

2. Monthly Popularity Index of a particular politician by country affiliation. Here, too, the classification is done separately for each response for the two approaches mentioned. For each relevant response from the response list, the emotion expressed in it will be categorized similar to the general monthly index, but the state from which the response writer comes will be taken into account, based on the position field in the response file record. Thus, the distribution of emotions expressed in responses according to affiliation to the United States is calculated and the results are presented in a table describing the distribution of opinions in each country as a percentage.

3. Index of sub-monthly popularity by analyzing the responses collected on a particular politician over a one-month period, ie 10 days. In this index, each month will be divided into first, second, and third trimester and in each such third, the distribution of opinions for each classification method will be calculated separately. Each entry in the comment file has a field that states the date of publication of the response, which allows the responses to be sorted in chronological order of writing, thus effectively dividing each month into three parts and allowing the distribution of emotions in responses to be shorter in more than one month, for only 10 days.
We will now explain how the classification methods we use will be explained:
Lexicon-based method: This method classifies text according to predefined words that appear in it. This method relies on lexicon indicative words to express English language emotions, with each word appearing a certain score. Each realization of the method computes differently the sentiment expressed in a particular text given as input. In order to perform lexicon-based classification, we have chosen to use the textblob directory which holds a TextBlob object that allows a variety of actions to be performed on text. We will use the polarity function under the Sentiment class of the TextBlob object. This function receives a string of text and returns a tupple whose left limb is a real number from 1 to 1 expressing the emotion expressed in the text, with 1- expressing a highly negative emotion, 1 expressing a very positive emotion, and 0 expressing a neutral emotion. This method uses a module called pattern.en which holds a lexicon of adjectives (such as good, bad, amazing, irritating) that appear frequently in the text. For each such word, a predefined score is attached, which varies by being a word that expresses a positive / negative emotion, with the score given to a particular text based on the adjectives it contains. Text classification that represents a response using this method will be as follows: In receiving some text, a TextBlob object consisting of that object is created. At this point the polarity function of the sentiment class will be run, which will return a value between 1- and 1. For the returned value, five levels of emotion will be defined: extremely positive (between 0.3 and 1), slightly positive (between 0 and 0.3), neutral (0), slightly negative (between 0.3 and 0), and most negative (between 1- To 0.3). 5 counters will be defined for each of the values, and this function will be run on each of the responses received as relevant to each of the politicians. Thus, for each set of responses in each category listed above (full-month response emotions, by country affiliation, and by sub-month breakdown), the percentage distribution of opinions will be calculated according to the scale set out above and the results presented in pie chart form.

Machine learning-based method: In this classification method, no lexicon is based on predefined indicative words but on machine learning-based methods, using classifiers to classify text that represents some response as expressing a positive emotion or negative emotion. This method involves two steps before we use it to run the New York Times comment file from which we will build the final index.

The training phase: The training phase is classified. To do this, we must use a pre-tagged data that we will trust the classifiers to. The data we will use will be Dataset # 3 detailed above in the form of 1,600,000 tweets for political / news events collected from Twitter. This tagged repository is suitable for use for our specific purpose, since these are responses to political / news events, which are similar in terms of vocabulary and style of writing and are in the same domain-text, which will help in training the classifieds in terms of selecting features used. We will do the training in three separate steps:
1. Noise Removal - The preprocessing phase of training. We'll take 15,000 positive and 15,000 negative, total of 30,000 responses (this number is determined by the RAM usage limit during training, more than GB16 consumes on your computer, which is not present on most PCs) ). We will define the part of speech of the words that will be for us pitchers, which will only be based on adjectives, nouns (including proper nouns) and verbs. Now we will build two lists of responses we took: a list consisting of all the words of the selected responses that are part of the POS we set (The rest of the words and punctuation will be thrown) when the letters in them are converted to lowercase, and another list of tupples, each tupple containing a comment and tagging Yes positive or negative We will take the 10,000 most common words from the list of words, which will form the basis for the next step, and save the two lists we created as reusable pickle files if necessary (saving the time needed to produce them each time).

2. Feature Selection - The feature creation stage. We will compile a list of tupples using the tupples list of comments and tags we created in the previous step. The right-hand organ in each tupple will label the response as positive or negative and the left-hand organ is a dictionary of 10,000 keys and values, with each key being one of the 10,000 most commonly found words and its value being a boolean variable representing whether that word is in the current response. This dictionary contains the features as boolean keys and variables as the value for each key that represents each word in the 10,000 most common words whether it is part of the current response, combined with the original tagging of the response. We'll now mix this list randomly, then compile 80 percent of it (24,000 responses) as Training Data and 20 percent (6,000 responses) as Testing data.

3. Sentiment Extraction - Define 5 different classifiers that will use supervised learning for the Training Data we created. We will use numeric scikit-learn classifiers and are: Naive Bayes, Multinomial Naive Bayes, Bernoulli Naive Bayes, Logistic Regression and Linear Support Vector. Trust each of them to the Training Data and then save them as quick reusable pickle files without the need for repetition each time, a fact that will save a lot of time when we run them for comment from the New York Times.
Testing Step: Now we will apply each of the five classifiers we built on the Testing Data to calculate the accuracy of each classifier. In this metric, calculate the percentages that Justice classified from all the ground truths of the responses in the Testing Set:
Testing Data Accuracy (%) Classifier
78.45 Naive Bayes
79.11 Multinomial Naive Bayes
79.23 Bernoulli Na√Øve Bayes
81.95 Logistic Regression
84.68 Linear Support Vector

Reminded that these are the results for the Testing Data tagged from the Twitter tweet (used for training classified and tagged accordingly) and not the NYT comment files from which we will build the politicians' popularity metrics (which do not contain any tags).
Once we are done with the training and initial examination of the tagged files, we will use the classifiers we prepared for the relevant NYT responses. The ML responses will be categorized into the categories on which the lexicon classification and distribution spread over a full month, based on state affiliation and subdivision. Because in the Twitter repository, the comments were labeled as positive or negative only (there are no comments labeled as neutral), any classifier that will trigger a comment will be whether it has only positive or negative emotion. Therefore, in receiving any text that represents a specific response, we shall state that the classification algorithm will be run in the voting system model configuration: Instead of receiving 5 different final answers from each of the classifiers for a response, we will define a voting system where each classifier gives an answer whether positive or negative is expressed. If there is at least 80% agreement on the emotion in the response, ie 4 out of 5 classifiers return the same result, the response will be classified accordingly as having a positive or negative emotion. If there is less agreement, that is, 3 classifieds give one result and 2 another, the reaction will be classified as neutral. For this reason, the value scale in this method for each response will include 3 levels: a response that expresses a positive, neutral, or negative emotion. To build a representative sample of randomly sampled independent responses that will be used to build the popularity metric, in each category we will randomly mix the list of responses we would like to analyze and also limit the number of samples to a maximum of 6,000, as the Testing Set size we used for accuracy. Tagged from Twitter. Similar to the lexicon method, for each category, the distribution of emotions expressed in percent responses will be calculated according to the value scale indicated above and the results presented in pie chart form.

Experiments:

Evaluation Criteria:
In general, the evaluation criteria for models that include text categorization according to emotion can include metrics such as Accuracy, Precision, Recall and F-score with which the test model performance can be assessed. All of these require tagged data with the help of measuring each one.
Because we build long-term popularity metrics for politicians based on untagged NYT repository, our evaluation criterion will be based on comparing US and national research and political / news / current trends and in-depth public opinion studies Reviewers are politicians, and the main bodies we will compare to the results we have received will be, for example, new companies, newspaper systems, strategic and business consulting companies, markets and research institutes and universities.
Since these bodies are skilled in conducting in-depth political research and trends in society, we will compare the results we get to the results published by them in order to evaluate the results we received and make sure they are not the case.
It should be noted that at the time we examined the Testing Data classifier from Twitter that was tagged (but not used to build the final popularity index), our evaluation criterion did include the Accuracy metric and the results are listed in the table above.

Setup:
First, we will run the classifier training on the file with the examples tagged as above and save them for quick reuse in the folder we are working with the pickle files, a fact that will save the need to retrain every time you run.
We will perform the experiments themselves chronologically according to the 3 categories listed above in our Solution section. The order of the politicians will be Donald Trump, Hillary Clinton, Barack Obama and Benjamin Netanyahu in both classification methods. Each politician in each of the month order classification methods will be January-May 2017 and then January-April 2018. The order of running within each month will be the collection of articles and articles related to the same politician that month, gathering user comments for the same articles, calculating the distribution of emotions in response to the whole month, calculating the distribution The emotions in the reactions that month according to state affiliation and finally the distribution of the emotions in the reactions according to each month divided into three parts and the distribution of the first, second and third parts of each month separately. At the end of the process, each politician, in each classification method each month, will be created into the three categories of graphs describing the distribution of reaction emotions in the form of a pie chart that we will present in the following section.

Results:
From the process described in the previous section, we will generate a large number of graphs that are short of containing all of this report. All graphs will be uploaded to Google Drive under the link posted in the project forum along with all code and data. Therefore, in this section, we will present selected results of all types along with the data with which The comparison and at the top of each result is a brief description of what it presents, followed by an explanation of the difference between lexicon-based and ML-based results.

Distribution of emotions in the last trimester of January 2017 for Donald Trump: right lexicon classification and left ML classification. Below are results from Wikipedia of several US research institutes this month:



